1. learning rate 10^-2 跑 1000 epoch 約200epoch就可以 test落在0.82
2. drop 0.25 0.5 落在0.86
3. drop 0.5 0.5 落在0.86
4. F1 16->8 F2 32->16 train落在0.86 test落在0.8
5. drop 改成 0.25 0.5 0.9/0.8
6. drop 改成 0.25 0.25 0.92/0.84 感覺準確度更上升一點(i think 這樣model才夠robust) 數值更接近彼此
7. batch size 64改256 (0.9398148148148148, 0.8324074074074074)
8. batch size 256改512 (0.8944444444444445, 0.7907407407407407) 準確度又掉下來 可能要更多epoch
9. 網路上看到BATCH SIZE越大會陷入sharp minima泛化性不好 batchsize 64->16 (0.8962962962962963, 0.7296296296296296)更爛
10. batch改 32 (0.9212962962962963, 0.8018518518518518)
11. weight decay 10^-5 (0.924074074074074, 0.7962962962962963) 差距還是很大
12. 10^-5 -> 10^-3 加上去之後更難學 但是兩者很接近 (0.8314814814814815, 0.8101851851851852) 想提升model robust
13. 改f1 16 f2 32(0.8490740740740741, 0.8296296296296296)
14. 改f1 32 f2 64(0.8462962962962963, 0.8018518518518518) 看來沒提升多少
15. 給更多batch size提供更多資訊 32 -> 256 (0.9462962962962963, 0.8018518518518518) 只有training提升 再加一點decay
16. 10^-3 -> 10^-2 (0.8351851851851851, 0.7768518518518519) 又貼近了 但training又掉
17. 5 * 10^-3 (0.850925925925926, 0.8)
18. 2 * 10^-3 (0.8972222222222223, 0.8083333333333333)
19. 3 * 10^-3 (0.8657407407407407, 0.7981481481481482)

backto 13.
20. (0.8203703703703704, 0.7851851851851852) 1000epoch
21. f1 32 f2 64 (0.8324074074074074, 0.7925925925925926) 震盪變更大 而且沒改善 1000epoch
22. batchsize 32-> 512 (0.9074074074074074, 0.7416666666666667)
23. 改成 drop 0.5 0.5 (0.8722222222222222, 0.8287037037037037)
24. 改成 drop 0.25 0.5 (0.9148148148148149, 0.8527777777777777)
25. return best model 0.9185185185185185, 0.8388888888888889 acc: 0.8546296296296296, loss: 0.42131322622299194

26. lr = 10^-3

best: test acc: 0.8731481481481481, test loss: 0.416285236676534
hyper: batchsize512
0.873
0.875
0.8759
0.873
0.874

1. 25->8 0.83 0.79 很接近但波動很大 且 acc下降
2. weight decay 1e-3 -> 1e-4  0.848 0.809 波動很大降低lr看看
3. lr 1e-3->1e-4 沒用
4. weight decay 取消-> 0.78 0.76沒用 覺得是模型不夠強
5. 8->16

## 25filter lr1e-3 noweight_decay 0.9203 0.8185
##1 改lr1e-4 0.85 0.76 感覺更新不夠多次 調整batch size與epoch
##2 200epoch 512batchsize->128 發現少量來更新波動比較不大 0.82 0.777 且 沒overfit 嘗試robust model
##3 epoch先調整上去 看200->1000後續有沒有增高 0.9 0.781 testing卡住了 overfit
##4 加上 regulization 1e-4 0.9037 0.789
##5 1e-4-> 1e-3 0.934 0.826
##6 1e-3-> 1e-2 0.946 0.8472
##7 1e-2-> 1e-1 0.91 0.850
##8 1e-2-> 5e-1 0.858 0.8351
##9 太陡 lr 1e-4->1e-5 想讓他持續上升 0.849 0.802 發現還在上升 epoch不夠
##10 epoch 1000->1300 0.8759 0.8259 發現仍在上升 但epoch太多了 改batchsize 
##11 batchsize 128->64 0.90 0.819
##12 batchsize 64->32 0.9046 0.8509
##13 覺得是lr太低了 1e-5改5e-5  梯度消失
##14 lr 5e-5 -> 2e-5 發現差不多這裡就是頂 0.876 0.84
##15 regulization放寬一點 5e-1 -> 2e-1

0.9537 0.856 //0.8509



